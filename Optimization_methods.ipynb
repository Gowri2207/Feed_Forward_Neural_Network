{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb8dff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ea16811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,X_val,Y_val,activation_func, loss_func, eta, num_epochs, num_hidden_layers, sizes, batch_size,inputsize,outputsize):\n",
    "    \n",
    "    print('Gradient_descent')\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    #param_after_every_epoch={}\n",
    "    epoch = 0\n",
    "    params = createnetwork(num_hidden_layers, activation_func, sizes, inputsize, outputsize)\n",
    "    #for every epoch\n",
    "    while epoch < num_epochs:\n",
    "        print(epoch)\n",
    "        \n",
    "        grads = creategrads(num_hidden_layers, sizes, inputsize,outputsize)\n",
    "        #in every epoch for every sample\n",
    "        sample_indices = gen_sample_indices(batch_size,X.shape[0])\n",
    "        for j in sample_indices:\n",
    "            x = X[j,:]  #taking jth sample\n",
    "            \n",
    "            y = Y[j,:]\n",
    "            \n",
    "            #forward propogation\n",
    "            yhat, A, H = fwd_prop(x, params, activation_func, num_hidden_layers)\n",
    "            \n",
    "            #Backward propogation for gradients\n",
    "            grad_current = back_prop(H,A, params, num_hidden_layers, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
    "            \n",
    "            #summing the grads\n",
    "            #print(grad_current)\n",
    "            for key in grads:\n",
    "                grads[key] = grads[key] + grad_current[key]\n",
    "                \n",
    "            #all grads are summed for all samples now we need to update the parameters\n",
    "            for newkey in params:\n",
    "                params[newkey] = params[newkey] - eta * (grads[\"d\"+newkey])\n",
    "        \n",
    "        #again restoring the grads dictionary for next epoch\n",
    "        grads = creategrads(num_hidden_layers, sizes, inputsize, outputsize)\n",
    "        \n",
    "        #Finding accuracy and loss for every epoch\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_err, train_loss, val_error, val_loss = measure_performance(X,Y, X_val, Y_val,params, activation_func, num_hidden_layers, loss_func)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(\"for epoch {} train error: {}  train loss:{}  validation error:{}  validation loss:{}\".format(epoch, train_err, train_loss, val_error, val_loss))\n",
    "        #print(params)\n",
    "        epoch +=1\n",
    "        \n",
    "    return train_loss_list, val_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858e3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b1dd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X,Y,X_val,Y_val,activation_func, loss_func, eta, num_epochs, num_hidden_layers, sizes, batch_size,inputsize,outputsize):\n",
    "    \n",
    "    print('Minibatch_Gradient_descent with batch size {}'.format(batch_size))\n",
    "    \n",
    "    #param_after_every_epoch={}\n",
    "    epoch = 0\n",
    "    epochs=[]\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    params = createnetwork(num_hidden_layers, activation_func, sizes, inputsize, outputsize)\n",
    "    #for every epoch\n",
    "    while epoch < num_epochs:\n",
    "        print(epoch)\n",
    "        pointsseen=0\n",
    "        grads = creategrads(num_hidden_layers, sizes, inputsize,outputsize)\n",
    "        #in every epoch for every sample\n",
    "        sample_indices = gen_sample_indices(batch_size,X.shape[0])\n",
    "        for j in sample_indices:\n",
    "            x = X[j,:]  #taking jth sample\n",
    "            \n",
    "            y = Y[j,:]\n",
    "            \n",
    "            #forward propogation\n",
    "            yhat, A, H = fwd_prop(x, params, activation_func, num_hidden_layers)\n",
    "            \n",
    "            #Backward propogation for gradients\n",
    "            grad_current = back_prop(H,A, params, num_hidden_layers, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
    "            \n",
    "            #summing the grads\n",
    "            #print(grad_current)\n",
    "            for key in grads:\n",
    "                grads[key] = grads[key] + grad_current[key]\n",
    "            pointsseen+=1    \n",
    "            #all grads are summed for all samples now we need to update the parameters\n",
    "            if(pointsseen%batch_size == 0):\n",
    "                for newkey in params:\n",
    "                    params[newkey] = params[newkey] - eta * (grads[\"d\"+newkey])\n",
    "        \n",
    "        #again restoring the grads dictionary for next epoch\n",
    "        grads = creategrads(num_hidden_layers, sizes, inputsize, outputsize)\n",
    "        \n",
    "        #Finding accuracy and loss for every epoch\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_err, train_loss, val_error, val_loss = measure_performance(X,Y, X_val, Y_val,params, activation_func, num_hidden_layers, loss_func)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        epochs.append(epoch)\n",
    "        print(\"for epoch {} train error: {}  train loss:{}  validation error:{}  validation loss:{}\".format(epoch, train_err, train_loss, val_error, val_loss))\n",
    "        #print(params)\n",
    "        epoch +=1\n",
    "        \n",
    "    return train_loss_list, val_loss_list,epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afcc947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mgd(X, Y, X_val, Y_val, activation_func, loss_func, eta, gamma, num_epochs, num_hidden, sizes, batch_size,\n",
    " inputsize, outputsize):\n",
    "    print(\"momentum gradient descent\")\n",
    "    # data for information display and plots\n",
    "    step_data = {}\n",
    "    epoch_data = []\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    epochs =[]\n",
    "\n",
    "    params = createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
    "\n",
    "\n",
    "\n",
    "    prev_momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "    momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "    pointsseen = 0\n",
    "\n",
    "    epoch = 0\n",
    "    while epoch < num_epochs:\n",
    "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
    "        step = 0\n",
    "        sample_indices = gen_sample_indices(batch_size,X.shape[0])\n",
    "        for j in sample_indices:\n",
    "            x = X[j,:]\n",
    "            y = Y[j,:]\n",
    "            yhat, A, H = fwd_prop(x, params, activation_func, num_hidden)\n",
    "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
    "            for key in grads:\n",
    "                grads[key] = grads[key] + grad_current[key]\n",
    "\n",
    "            pointsseen = pointsseen + 1\n",
    "\n",
    "            # check if one step is done, update parameters, initialize new gradients\n",
    "            if pointsseen% batch_size == 0:\n",
    "                for newkey in params:\n",
    "            # TODO: (0) add batch_size wala division\n",
    "                    momenta[\"v\" + newkey] = gamma*prev_momenta[\"v\" + newkey] + eta*grads[\"d\" + newkey]\n",
    "                    params[newkey] = params[newkey] - momenta[\"v\" + newkey]\n",
    "                    prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey]\n",
    "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
    "\n",
    "                step = step + 1\n",
    "                # store data for log files if 100 steps are done\n",
    "        train_err, train_loss, val_error, val_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
    "        print(\"for epoch {} train error: {}  train loss:{}  validation error:{}  validation loss:{}\".format(epoch, train_err, train_loss, val_error, val_loss))\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        epochs.append(epoch)\n",
    "        epoch = epoch + 1\n",
    "\n",
    "    return train_loss_list, val_loss_list, epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e3867122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(X, Y, X_val, Y_val, activation_func, loss_func, eta, num_epochs, num_hidden, sizes,batch_size,inputsize, outputsize, beta1, beta2, eps):\n",
    "    print('Adam Optimization')\n",
    "    \n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    epochs_list=[]\n",
    "    \n",
    "    params=createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
    "    \n",
    "    prev_momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "    momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "    momenta_hat = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "    \n",
    "    prev_momenta_squared = createmomenta_squared(num_hidden, sizes, inputsize, outputsize)\n",
    "    momenta_squared = createmomenta_squared(num_hidden, sizes, inputsize, outputsize)\n",
    "    momenta_squared_hat = createmomenta_squared(num_hidden, sizes, inputsize, outputsize)\n",
    "    \n",
    "    pointsseen = 0\n",
    "    epoch = 0\n",
    "    \n",
    "    while epoch < num_epochs:\n",
    "        step=0\n",
    "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
    "        \n",
    "        sample_indices = gen_sample_indices(batch_size,X.shape[0])\n",
    "        for j in sample_indices:  #TODO: Problem here is at every epoch we are selecting same hundered epochs\n",
    "            x=X[j,:]   #TODO: even if we run this loop for X.shape[0] we are just considering the parameters updated after 100 samples\n",
    "            y=Y[j,:]   #TODO: generate 100 random integers for every epoch in the range of 0 - X.shape[0] and run for those idices (100 times) and update parameters\n",
    "            \n",
    "            yhat, A, H = fwd_prop(x, params, activation_func, num_hidden)\n",
    "            \n",
    "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
    "            \n",
    "            for key in grads:\n",
    "                grads[key] = grads[key]+grad_current[key]\n",
    "                \n",
    "            pointsseen+=1\n",
    "            \n",
    "            if pointsseen%batch_size == 0:\n",
    "                step =step+1\n",
    "                for newkey in params:\n",
    "                    momenta[\"v\"+newkey] = beta1*prev_momenta[\"v\"+newkey]+(1-beta1)*grads[\"d\"+newkey]\n",
    "                    momenta_squared[\"m\"+newkey] = beta2*prev_momenta_squared[\"m\"+newkey]+(1-beta2)*((grads[\"d\"+newkey])**2)\n",
    "                    \n",
    "                    momenta_hat[\"v\" + newkey] = momenta[\"v\" + newkey]/(1 - np.power(beta1, step))\n",
    "                    momenta_squared_hat[\"m\" + newkey] = momenta_squared[\"m\" + newkey]/(1 - np.power(beta2, step))\n",
    "\n",
    "                    params[newkey] = params[newkey] - (eta/np.sqrt(momenta_squared_hat[\"m\" + newkey] + eps))*momenta_hat[\"v\" + newkey]\n",
    "\n",
    "                    prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey]\n",
    "                    prev_momenta_squared[\"m\" + newkey] = momenta_squared[\"m\" + newkey]\n",
    "                \n",
    "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
    "                \n",
    "        train_err, train_loss, val_err, val_loss= measure_performance(X, Y, X_val, Y_val,params, activation_func, num_hidden, loss_func)\n",
    "        print(\"for epoch {} train error: {}  train loss:{}  validation error:{}  validation loss:{}\".format(epoch, train_err, train_loss, val_err, val_loss))\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        val_loss_list.append(val_loss)\n",
    "        epochs_list.append(epoch)\n",
    "        epoch+=1\n",
    "    return train_loss_list, val_loss_list, epochs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "13be9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nag(X, Y, X_val, Y_val, activation_func, loss_func, eta, gamma, num_epochs, num_hidden, sizes, batch_size,\n",
    " inputsize, outputsize):\n",
    "    print(\"NAG\")\n",
    "    epoch_list = []\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "\n",
    "    # initializing\n",
    "\n",
    "    params = createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
    "\n",
    "    prev_momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "    momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
    "\n",
    "    pointsseen = 0\n",
    "    epoch = 0\n",
    "    while epoch < (num_epochs):\n",
    "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
    "        step = 0\n",
    "        sample_indices = gen_sample_indices(batch_size,X.shape[0])\n",
    "        for j in sample_indices:\n",
    "            x = X[j,:]\n",
    "            y = Y[j,:]\n",
    "            yhat, A, H = fwd_prop(x, params, activation_func, num_hidden)\n",
    "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
    "            for key in grads:\n",
    "                grads[key] = grads[key] + grad_current[key]\n",
    "\n",
    "            pointsseen = pointsseen + 1\n",
    "\n",
    "            # check if one step is done, update parameters, initialize new gradients\n",
    "            if pointsseen% batch_size == 0:\n",
    "                step = step + 1\n",
    "\n",
    "                for newkey in params:\n",
    "            # TODO: (0) add batch_size wala division\n",
    "                    momenta[\"v\" + newkey] = gamma*prev_momenta[\"v\" + newkey] + eta*grads[\"d\" + newkey]\n",
    "                    params[newkey] = params[newkey] - momenta[\"v\" + newkey]\n",
    "                    prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey]\n",
    "\n",
    "\n",
    "\n",
    "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
    "                \n",
    "                # TODO: review what happens in the last step\n",
    "                for next_key in params:\n",
    "                    momenta[\"v\" + next_key] = gamma*prev_momenta[\"v\" + next_key]\n",
    "                    params[next_key] = params[next_key] - momenta[\"v\" + next_key]\n",
    "\n",
    "        train_err, train_loss, val_err, val_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
    "        print(\"for epoch {} train error: {}  train loss:{}  validation error:{}  validation loss:{}\".format(epoch, train_err, train_loss, val_err, val_loss))\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        epoch_list.append(epoch)\n",
    "\n",
    "\n",
    "        epoch = epoch + 1\n",
    "    return train_loss_list, val_loss_list, epoch_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e49cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db935e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a00e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6b0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228731c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
