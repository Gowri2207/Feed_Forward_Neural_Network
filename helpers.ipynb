{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a3848f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(num_hidden_layers, activation_function, sizes, inputsize, outputsize): #here input size and output size are fixed as per problem this can be modified as per requirement\n",
    "\n",
    "  sizes = [inputsize] + sizes\n",
    "\n",
    "  sizes = sizes+ [outputsize]\n",
    "\n",
    "  np.random.seed(1234)\n",
    "\n",
    "  parameters={}\n",
    "\n",
    "  if activation_function == \"relu\":\n",
    "    parameters[\"W\" + str(i)] = 0.01*np.random.randn(sizes[i], sizes[i-1])*(np.sqrt(2/(sizes[i]+sizes[i-1])))\n",
    "    parameters[\"b\" + str(i)] = np.zeros(sizes[i],1)\n",
    "\n",
    "  else:\n",
    "    for i in range(1, num_hidden_layers+2):\n",
    "      parameters[\"W\" + str(i)] = np.random.randn(sizes[i], sizes[i-1])   #initializing weight matrix\n",
    "      parameters[\"b\" + str(i)] = np.zeros(sizes[i],1)   #initializing bias vector\n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "aeeaba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_prop(X, parameters, activation_func, num_hidden_layers):\n",
    "\n",
    "    A={}\n",
    "    activation = Activation(activation_func)\n",
    "    if X.ndim == 1:\n",
    "      X=X[:,np.newaxis]\n",
    "\n",
    "    H = {\"h0\":X}\n",
    "\n",
    "    for l in range(1,num_hidden_layers+2):\n",
    "      Wl=parameters[\"W\"+str(l)]\n",
    "      bl=parameters[\"b\"+str(l)]\n",
    "\n",
    "      hprev = H[\"h\" + str(l-1)]\n",
    "      al = np.dot(Wl, hprev) +bl\n",
    "      A[\"a\"+str(l)] = al\n",
    "\n",
    "      if l!= num_hidden_layers+1:\n",
    "        hl = activation.activate(al)\n",
    "\n",
    "      elif l == num_hidden_layers+1:\n",
    "        hl=softmax(al)\n",
    "\n",
    "      H[\"h\"+str(l)] = hl\n",
    "\n",
    "\n",
    "    yhat=H[\"h\"+str(num_hidden_layers+1)]\n",
    "\n",
    "    return yhat, A, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "477adbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating loss\n",
    "class loss_class:\n",
    "    def __init__(self, loss_method):\n",
    "        self.loss_method = loss_method\n",
    "        \n",
    "    def calc_loss(self, y_pred, y_act):\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_act = np.array(y_act)        \n",
    "\n",
    "        if y_pred.ndim == 1:\n",
    "            y_pred = y_pred[:,np.newaxis]\n",
    "\n",
    "        if y_act.ndim == 1:\n",
    "            y_act = y_act[:, np.newaxis]\n",
    "        \n",
    "        if self.loss_method == 'cross_entropy':\n",
    "            loss = -1*y_act*np.log(y_pred)\n",
    "            loss = np.sum(loss)\n",
    "            #print(loss)\n",
    "        elif self.loss_method == 'squared_loss':\n",
    "            loss = 0.5*np.sum((y_pred - y_act)**2)\n",
    "      \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d8be207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "  z = a-np.max(a)   #check why is this necessary\n",
    "  numerator = np.exp(z)\n",
    "  denom = np.sum(numerator, axis = 0)\n",
    "\n",
    "  return numerator/denom    # returns the list of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8f0120f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "  def __init__(self, acti ):\n",
    "    self.acti = acti\n",
    "\n",
    "\n",
    "\n",
    "  def activate(self, a, derivative = False):\n",
    "    if derivative == False:\n",
    "      if self.acti == 'sigmoid':\n",
    "        return 1/(1+np.exp(-a))\n",
    "\n",
    "      elif self.acti == 'tanh':\n",
    "        return np.tanh(a)\n",
    "\n",
    "      elif self.acti == 'relu':\n",
    "        return ((a>0)*(a))+((a<0)*(0.01)*a)      #leaky relu\n",
    "\n",
    "\n",
    "\n",
    "    elif derivative == True:\n",
    "      if self.acti == 'sigmoid':\n",
    "        sig = 1/(1+np.exp(-a))\n",
    "        return sig*(1-sig)\n",
    "\n",
    "\n",
    "      elif self.acti == 'tanh':\n",
    "        return (1-((np.tanh(a)**2)))\n",
    "\n",
    "      elif self.acti == 'relu':\n",
    "        return (a>0)*(np.ones(np.shape(a))) + (a<0)*(0.01*np.ones(np.shape(a)))     #leaky relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bd2c70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(H, A, parameters, num_hidden_layers, sizes, Y, Yhat, loss, activation_func, inputsize, outputsize):\n",
    "\n",
    "  #gradient wrt a_L(outp-put-pre-activation)\n",
    "  activation = Activation(activation_func)\n",
    "  # first step in the pseudo code\n",
    "  grad_one_eg = creategrads(num_hidden_layers, sizes, inputsize , outputsize)\n",
    "\n",
    "  A[\"a0\"] = np.zeros((inputsize,1))\n",
    "\n",
    "  if Y.ndim == 1:\n",
    "      Y = Y[:, np.newaxis]\n",
    "  if Yhat.ndim == 1:\n",
    "      Yhat = Yhat[:, np.newaxis]\n",
    "    \n",
    "  if loss == \"cross_entropy\" :\n",
    "    #print(Yhat-Y)\n",
    "    grad_one_eg[\"da\"+str(num_hidden_layers+1)] =np.around(Yhat - Y,4)\n",
    "    #print(grad_one_eg)\n",
    "\n",
    "  elif loss == 'squared_loss':\n",
    "    grad_one_eg[\"da\" + str(num_hidden_layers+1)] = (Yhat - Y)*Yhat - Yhat*(np.dot((Yhat - Y).T, Yhat))\n",
    "\n",
    "  #second step in psuedo code\n",
    "  for i in np.arange(num_hidden_layers + 1, 0, -1):\n",
    "    #gradient of weights\n",
    "    grad_one_eg[\"dW\"+str(i)] = np.dot(grad_one_eg[\"da\"+str(i)], (H[\"h\"+str(i-1)]).T)\n",
    "\n",
    "    #gradient of biases\n",
    "    grad_one_eg['db' + str(i)] = grad_one_eg[\"da\" + str(i)]\n",
    "\n",
    "\n",
    "    grad_one_eg[\"dh\"+str(i-1)] = np.dot((parameters[\"W\" + str(i)]).T , grad_one_eg[\"da\"+str(i)])\n",
    "\n",
    "    g_dash = activation.activate(A[\"a\" + str(i-1)], derivative = True)\n",
    "\n",
    "    grad_one_eg[\"da\" + str(i-1)] = (grad_one_eg[\"dh\" + str(i-1)])*g_dash\n",
    "  #print(grad_one_eg)\n",
    "  return grad_one_eg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "333ace10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creategrads(num_hidden_layers, sizes, inputsize, outputsize):\n",
    "\n",
    "  sizes = [inputsize] + sizes\n",
    "\n",
    "  sizes = sizes + [outputsize]\n",
    "  \n",
    "  grads={\"da0\":np.zeros((inputsize,1)),\"dho\":np.zeros((inputsize,1))}\n",
    "\n",
    "  for i in range(1, num_hidden_layers+2):\n",
    "        grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
    "        grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
    "        grads[\"da\" + str(i)] = np.zeros((sizes[i],1))\n",
    "        grads[\"dh\" + str(i)] = np.zeros((sizes[i],1))\n",
    "\n",
    "  return grads\n",
    "\n",
    "  #initializing all gradient matrices with zero to fill up while back propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f0e87770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createnetwork(num_hidden_layers, activation_func, sizes, inputsize, outputsize):\n",
    "    sizes = [inputsize]+sizes+[outputsize]\n",
    "    np.random.seed(1234)\n",
    "    parameters = {}\n",
    "          \n",
    "    if activation_func == 'relu':\n",
    "        for i in range(1,num_hidden_layers+2):\n",
    "            parameters['W'+str(i)] = 0.01*np.random.randn(sizes[i],sizes[i-1])*(np.sqrt(2/sizes[i]+sizes[i-1]))\n",
    "            parameters['b'+str(i)] = np.zeros((sizes[i],1))\n",
    "        \n",
    "    else:\n",
    "        for i in range(1, num_hidden_layers+2):\n",
    "            parameters['W'+str(i)] = np.random.randn(sizes[i],sizes[i-1])\n",
    "            parameters['b'+str(i)] = np.random.randn(sizes[i],1)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "80c76d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(X,Y, X_val, Y_val, params, activation_func, num_hidden_layers, loss):\n",
    "    #we got final parameters\n",
    "    #find Y_pred\n",
    "    Yhat= np.zeros((Y.shape[0],Y.shape[1]))\n",
    "    for j in range(0,X.shape[0]):\n",
    "        x=X[j,:]\n",
    "        y,_,_=fwd_prop(x,params,activation_func, num_hidden_layers)\n",
    "        #print(y.shape)\n",
    "        y=y.reshape(25,)\n",
    "        Yhat[j,:]=y\n",
    "    \n",
    "        \n",
    "    #Yhat, _, _ = fwd_prop(X, params, activation_func, num_hidden_layers)\n",
    "    \n",
    "    #print(Yhat.shape)\n",
    "    #print(Y.shape)\n",
    "    train_acc = find_accuracy(Yhat, Y)\n",
    "    train_err = 100 - train_acc\n",
    "    loss = loss_class(loss)\n",
    "    train_loss = loss.calc_loss(Yhat, Y)\n",
    "    train_loss = train_loss/float(X.shape[0])\n",
    "    Yhat_val=np.zeros((Y_val.shape[0],Y_val.shape[1]))\n",
    "    for j in range(0,X_val.shape[0]):\n",
    "        x_val=X_val[j,:]\n",
    "        y_val,_,_=fwd_prop(x,params, activation_func, num_hidden_layers)\n",
    "        y_val=y_val.reshape(25,)\n",
    "        Yhat_val[j,:] = y_val\n",
    "    #print(Yhat_val)\n",
    "    val_acc = find_accuracy(Yhat_val, Y_val)\n",
    "    \n",
    "    val_err = 100 - val_acc\n",
    "    val_loss = loss.calc_loss(Yhat_val,Y_val)\n",
    "    n=X_val.shape[0]\n",
    "    #print(val_loss)\n",
    "    val_loss=val_loss/float(n)\n",
    "    '''print(train_err)\n",
    "    print(train_loss)\n",
    "    print(val_err)\n",
    "    print(val_loss)'''\n",
    "    return train_err, train_loss, val_err, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b7a5b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(yhat,y):\n",
    "    a = np.argmax(yhat, axis =0)\n",
    "    \n",
    "    b = np.argmax(y, axis =0)\n",
    "    \n",
    "    return 100*(np.sum( a == b)/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b195d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotfigure(xlabel,ylabel,title, x, y=[], figsize=(10,10), style = \"k-\", graph = \"plot\"):\n",
    "  plt.figure(figsize = figsize)\n",
    "  plt.grid(True)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.title(title)\n",
    "\n",
    "  if(len(y) == 0):\n",
    "    plt.plot(x,style)\n",
    "\n",
    "  else:\n",
    "    if graph == \"plot\":\n",
    "      plt.plot(x,y,style)\n",
    "    if graph == \"semilogx\":\n",
    "      plt.semilogx(x,y,style)\n",
    "    if graph == \"semilogy\":\n",
    "      plt.semilogy(x,y,style)\n",
    "    if graph == \"loglog\":\n",
    "      plt.loglog(x,y,style)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c1a32a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_indices(batch_size,n):\n",
    "  list_of_numbers = []\n",
    "  for i in range(0,batch_size+1):\n",
    "    list_of_numbers.append(random.randint(0, n))\n",
    "\n",
    "  return list_of_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "61f40374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_params(file_name):\n",
    "    file = open(file_name ,\"r\")\n",
    "    lines = file.readlines()\n",
    "    hyper_parameters={}\n",
    "    for i,line in enumerate(lines):\n",
    "        value = line.strip()\n",
    "        value = value.split()\n",
    "        hyper_parameters[value[0]]=value[1]\n",
    "    return hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "25b391f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_to_train, path_to_test):\n",
    "    data = pd.read_csv(path_to_train)\n",
    "    X=data.iloc[:,1:]\n",
    "    X=X/255 #normalizing the pixel values   num samples x 784\n",
    "    indices = data.iloc[:,0]\n",
    "    Y = (convert_to_onehot(indices,25)) #num samples x 25\n",
    "    pca = PCA(n_components=50) \n",
    "    pca.fit(X)\n",
    "    #splitting the data to train and validation data\n",
    "    \n",
    "    X_train,X_val,y_train,y_val = train_test_split(X,Y,test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    X_train = pca.transform(X_train)\n",
    "    \n",
    "    X_val = pca.transform(X_val)\n",
    "    \n",
    "    data_test = pd.read_csv(path_to_test)\n",
    "    X_test=data_test.iloc[:,1:]\n",
    "    X_test = X_test/255\n",
    "    X_test = pca.transform(X_test)\n",
    "    indices_test = data.iloc[:,0]\n",
    "    y_test = convert_to_onehot(indices,25)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c129837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(indices, num_classes):\n",
    "\n",
    "  output = np.eye(num_classes)[np.array(indices).reshape(-1)]\n",
    "\n",
    "  return output.reshape(list(np.shape(indices))+[num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a8beacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmomenta(num_hidden, sizes, inputsize, outputsize):\n",
    "    sizes = [inputsize] + sizes\n",
    "    sizes = sizes + [outputsize]\n",
    "    momenta = {}\n",
    "    for i in range(1, num_hidden+2):\n",
    "        momenta[\"vW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
    "        momenta[\"vb\" + str(i)] = np.zeros((sizes[i],1))\n",
    "        # TODO: (5) scale these by 0.01 like in andrew ng's course?\n",
    "\n",
    "    return momenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dff8ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmomenta_squared(num_hidden, sizes, inputsize, outputsize):\n",
    "    sizes = [inputsize] + sizes\n",
    "    sizes = sizes + [outputsize]\n",
    "    momenta = {}\n",
    "    for i in range(1, num_hidden+2):\n",
    "        momenta[\"mW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
    "        momenta[\"mb\" + str(i)] = np.zeros((sizes[i],1))\n",
    "        # TODO: (5) scale these by 0.01 like in andrew ng's course?\n",
    "\n",
    "    return momenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416edf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc157a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d260e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
